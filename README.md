# JOILang: Natural Language–Driven IoT Automation DSL

This repository provides a reference implementation and benchmark resources for JOILang,
a service-level domain-specific language (DSL) for natural language–driven IoT automation.

The repository focuses on:
- Public release of the JOILang scripting language and the JOICommands-170 dataset
- A reproducible benchmark pipeline for NL→JOILang generation
- Clear separation between automatic evaluation and expert judgment

It is intended as a research reference and evaluation repository, not as a production system.

---

## Overview

JOILang enables IoT automation scripts to be written at the service level, decoupling automation logic
from concrete device identifiers. Instead of addressing individual devices, scripts operate over
abstract services and tag-based scopes such as location, role, or function.

This repository supports:
- Translating natural language commands into JOILang scripts
- Benchmarking different models and prompting strategies
- Storing expert-annotated correctness judgments in a transparent format

---

## Repository Structure

```
.
├── datasets/
│   └── JOICommands-170.json        # Benchmark dataset
│
├── prompts/
│   └── blocks/                    # Atomic prompt blocks
│
├── rag/
│   ├── query_router.py            # Service / concept pre-mapping
│   └── assemble_prompt.py         # Prompt assembly
│
├── generation/
│   └── generate.py                # Low-level NL→JOILang generation
│
├── evaluation/
│   ├── run_benchmark.py           # Generation + automatic evaluation
│   └── summarize_results.py       # Aggregate benchmark results
│
├── artifacts/
│   └── results.csv                # Benchmark outputs (generated)
│
├── run.py                         # Single-command NL→JOILang interface
├── requirements.txt
└── README.md
```

---

## Dataset: JOICommands-170

JOICommands-170 is a benchmark dataset consisting of 170 natural language IoT commands
(Korean and English), paired with expert-written JOILang reference scripts.

Each entry contains:
- id
- command
- gpt_gt: reference JOILang script

The dataset is intended for evaluation and analysis of NL→DSL generation systems.

---

## Evaluation Format

Benchmark results are stored in a single CSV file.

Each row corresponds to one command.

Core columns:
- id
- command
- gpt_gt
- <model_name>: JOILang script generated by the model
- <model_name>_S_DET: automatically computed deterministic score
- <model_name>_pass: expert-assigned pass/fail label

When a new model is evaluated, new columns are appended.
Existing annotations are never overwritten.

---

## Automatic and Human Evaluation

### Automatic evaluation (S_DET)

S_DET is computed automatically by the evaluation tools.
It reflects rule-level and structural correctness under JOILang semantics.
No human input is required for this step.

### Pass annotation (human-in-the-loop)

Pass columns are not automatically filled.
A domain expert compares the generated JOILang script with the reference script (gpt_gt).

A sample is marked as Pass if the scripts are semantically equivalent under JOILang execution semantics,
even if minor syntactic differences exist.

---

## Running the Benchmark

### Setup

```bash
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
export OPENAI_API_KEY="YOUR_API_KEY"
```

### Step 1: Run benchmark

```bash
python evaluation/run_benchmark.py \
  --dataset datasets/JOICommands-170.json \
  --model gpt-4.1-mini \
  --out artifacts/results.csv
```

This command generates JOILang scripts for all commands, computes S_DET,
and creates a corresponding pass column.

### Step 2: Human pass annotation

Open artifacts/results.csv and manually fill the <model_name>_pass column
based on expert comparison with gpt_gt.

---

## Aggregating Results

```bash
python evaluation/summarize_results.py \
  --input artifacts/results.csv \
  --out artifacts/summary_table.md
```

The summary includes pass rates and average S_DET values for each model.

---

## Interactive Usage

For single-command generation without running the full benchmark:

```bash
python run.py \
  --sentence "Turn off all heaters when I leave" \
  --model gpt-4.1-mini
```

The generated JOILang script is printed to standard output.

---

## Scope and Notes

This repository emphasizes evaluation clarity over automation.
No automatic semantic judge is applied to determine pass or fail.
This repository emphasizes evaluation clarity over automation, while providing
a minimal command-line interface for generating JOILang scripts from natural language.

---

## License and Citation

License: TBD

If you use this repository or the JOICommands-170 dataset,
please cite the corresponding paper.
